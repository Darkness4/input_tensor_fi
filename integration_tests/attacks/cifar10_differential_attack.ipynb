{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "requested-installation",
   "metadata": {},
   "source": [
    "# CIFAR-10 Differential Attack\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "portuguese-junior",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "optical-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = os.getcwd()\n",
    "MODEL_PATH = os.path.join(FILE_PATH, \"../models/my_vgg.h5\")\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-reply",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We work with categorical (binary class matrix) instead of class vectors (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cardiac-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def __prepare_datasets():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "executed-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (50000, 32, 32, 3) y_train.shape = (50000, 10)\n",
      "x_test.shape = (10000, 32, 32, 3) y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = __prepare_datasets()\n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "print(f\"x_train.shape = {x_train.shape} y_train.shape = {y_train.shape}\")\n",
    "print(f\"x_test.shape = {x_test.shape} y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-garbage",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "We use our own VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nominated-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from integration_tests.models.my_vgg import my_vgg\n",
    "\n",
    "def __prepare_model(data_train, data_test):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(\"---Using Existing Model---\")\n",
    "        model: tf.keras.Model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    else:\n",
    "        print(\"---Training Model---\")\n",
    "        print(f\"GPU IS AVAILABLE: {tf.config.list_physical_devices('GPU')}\")\n",
    "        model: tf.keras.Model = my_vgg()\n",
    "        model.fit(\n",
    "            *data_train,\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            validation_data=data_test,\n",
    "        )\n",
    "        model.save(MODEL_PATH)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "varying-mineral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Using Existing Model---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = __prepare_model(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-thanks",
   "metadata": {},
   "source": [
    "## Look for fragile images\n",
    "\n",
    "(Images that can be easily missclassified.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "developing-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_fragile_images(\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "    fragility_threshold=0.1,\n",
    "):\n",
    "    \"\"\"Look for images which are sensible to FI.\n",
    "\n",
    "    \"Fragile image\" has these conditions :\n",
    "\n",
    "    -  y_pred_index == y_true_index\n",
    "    -  std(y_pred) < fragility_threshold\n",
    "    \"\"\"\n",
    "    x_test, y_test = data_test\n",
    "    result = model.predict(x_test)\n",
    "\n",
    "    for i, y_pred in enumerate(result):\n",
    "        y_true = y_test[i]\n",
    "        y_true_index = np.argmax(y_true)\n",
    "        y_pred_index = np.argmax(y_pred)\n",
    "\n",
    "        if (\n",
    "            y_pred_index == y_true_index\n",
    "            and np.std(y_pred) < fragility_threshold\n",
    "        ):\n",
    "            print(\n",
    "                f\"image {i} is fragile.\\n\"\n",
    "                f\"std: {np.std(y_pred)}.\\n\"\n",
    "                f\"y_pred[y_true_index]={y_pred[y_true_index]}\\n\"\n",
    "                f\"y_pred[0]={y_pred[0]}\\n\"\n",
    "            )\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exempt-handy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 629 is fragile.\n",
      "std: 0.08297225087881088.\n",
      "y_pred[y_true_index]=0.3010726869106293\n",
      "y_pred[0]=0.11562445014715195\n",
      "\n",
      "image 878 is fragile.\n",
      "std: 0.09110098332166672.\n",
      "y_pred[y_true_index]=0.28201019763946533\n",
      "y_pred[0]=0.14032000303268433\n",
      "\n",
      "image 1975 is fragile.\n",
      "std: 0.09571870416402817.\n",
      "y_pred[y_true_index]=0.2684406340122223\n",
      "y_pred[0]=0.18283182382583618\n",
      "\n",
      "image 2032 is fragile.\n",
      "std: 0.0846942886710167.\n",
      "y_pred[y_true_index]=0.2672123312950134\n",
      "y_pred[0]=0.022510046139359474\n",
      "\n",
      "image 4282 is fragile.\n",
      "std: 0.09785398095846176.\n",
      "y_pred[y_true_index]=0.2889142632484436\n",
      "y_pred[0]=0.04903290793299675\n",
      "\n",
      "image 4705 is fragile.\n",
      "std: 0.08988039195537567.\n",
      "y_pred[y_true_index]=0.2513140141963959\n",
      "y_pred[0]=0.1394633948802948\n",
      "\n",
      "image 5700 is fragile.\n",
      "std: 0.08809789270162582.\n",
      "y_pred[y_true_index]=0.3044825792312622\n",
      "y_pred[0]=0.027622321620583534\n",
      "\n",
      "image 6083 is fragile.\n",
      "std: 0.07141569256782532.\n",
      "y_pred[y_true_index]=0.27099087834358215\n",
      "y_pred[0]=0.13926005363464355\n",
      "\n",
      "image 6729 is fragile.\n",
      "std: 0.08906044811010361.\n",
      "y_pred[y_true_index]=0.28118571639060974\n",
      "y_pred[0]=0.28118571639060974\n",
      "\n",
      "image 7491 is fragile.\n",
      "std: 0.08011260628700256.\n",
      "y_pred[y_true_index]=0.23264145851135254\n",
      "y_pred[0]=0.007820498198270798\n",
      "\n",
      "image 7735 is fragile.\n",
      "std: 0.09802649915218353.\n",
      "y_pred[y_true_index]=0.25223252177238464\n",
      "y_pred[0]=0.23485884070396423\n",
      "\n",
      "image 8428 is fragile.\n",
      "std: 0.09440340101718903.\n",
      "y_pred[y_true_index]=0.303862988948822\n",
      "y_pred[0]=0.05074286088347435\n",
      "\n",
      "image 8480 is fragile.\n",
      "std: 0.08550649881362915.\n",
      "y_pred[y_true_index]=0.3043674826622009\n",
      "y_pred[0]=0.07086876779794693\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fragile_imgs = list(__find_fragile_images(data_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-homework",
   "metadata": {},
   "source": [
    "## Differential Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-implementation",
   "metadata": {},
   "source": [
    "Our implementations is based on [github.com/Hyperparticle/one-pixel-attack-keras](https://github.com/Hyperparticle/one-pixel-attack-keras).\n",
    "\n",
    "```python\n",
    "def original_perturb_image(xs, img):\n",
    "    # If this function is passed just one perturbation vector,\n",
    "    # pack it in a list to keep the computation the same\n",
    "    if xs.ndim < 2:\n",
    "        xs = np.array([xs])\n",
    "\n",
    "    # Copy the image n == len(xs) times so that we can\n",
    "    # create n new perturbed images\n",
    "    tile = [len(xs)] + [1] * (xs.ndim + 1)\n",
    "    imgs = np.tile(img, tile)\n",
    "\n",
    "    # Make sure to floor the members of xs as int types\n",
    "    xs = xs.astype(int)\n",
    "\n",
    "    for x, img in zip(xs, imgs):\n",
    "        # Split x into an array of 5-tuples (perturbation pixels)\n",
    "        # i.e., [[x,y,r,g,b], ...]\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            # At each pixel's x,y position, assign its rgb value\n",
    "            x_pos, y_pos, *rgb = pixel\n",
    "            img[x_pos, y_pos] = rgb\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def predict_classes(\n",
    "    xs: np.ndarray, img: np.ndarray, y_true: int, model: tf.keras.Model\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perturb the image and get the predictions of the model.\"\"\"\n",
    "    imgs_perturbed = original_perturb_image(xs, img)\n",
    "    predictions = model.predict(imgs_perturbed)[:, y_true]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def attack_success(\n",
    "    x: np.ndarray,\n",
    "    img: np.ndarray,\n",
    "    y_true: int,\n",
    "    model: tf.keras.Model,\n",
    "    verbose=False,\n",
    ") -> Optional[bool]:\n",
    "    \"\"\"Predict ONE image and return True if expected. None otherwise.\"\"\"\n",
    "    attack_image = original_perturb_image(x, img)\n",
    "\n",
    "    confidence = model.predict(attack_image)[0]\n",
    "    predicted_class = np.argmax(confidence)\n",
    "\n",
    "    # If the prediction is what we want (misclassification or\n",
    "    # targeted classification), return True\n",
    "    if verbose:\n",
    "        print(\"Confidence:\", confidence[y_true])\n",
    "    if predicted_class == y_true:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def attack(\n",
    "    img: np.ndarray,\n",
    "    y_true: int,\n",
    "    model: tf.keras.Model,\n",
    "    pixel_count=1,\n",
    "    maxiter=75,\n",
    "    popsize=400,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Define bounds for a flat vector of x,y,r,g,b values\n",
    "    # For more pixels, repeat this layout\n",
    "    bounds = [(0, 32), (0, 32), (0, 256), (0, 256), (0, 256)] * pixel_count\n",
    "\n",
    "    # Population multiplier, in terms of the size of the perturbation vector x\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "\n",
    "    # Format the predict/callback functions for the differential evolution algorithm\n",
    "    def predict_fn(xs):\n",
    "        return predict_classes(xs, img, y_true, model)\n",
    "\n",
    "    def callback_fn(x, convergence):\n",
    "        return attack_success(\n",
    "            x,\n",
    "            img,\n",
    "            y_true,\n",
    "            model,\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "    # Call Scipy's Implementation of Differential Evolution\n",
    "    attack_result = differential_evolution(\n",
    "        predict_fn,\n",
    "        bounds,\n",
    "        maxiter=maxiter,\n",
    "        popsize=popmul,\n",
    "        recombination=1,\n",
    "        atol=-1,\n",
    "        callback=callback_fn,\n",
    "        polish=False,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # Calculate some useful statistics to return from this function\n",
    "        attack_image = original_perturb_image(attack_result.x, img)[0]\n",
    "        prior_probs = model.predict(np.array([img]))[0]\n",
    "        prior_class = np.argmax(prior_probs)\n",
    "        predicted_probs = model.predict(np.array([attack_image]))[0]\n",
    "        predicted_class = np.argmax(predicted_probs)\n",
    "        success = predicted_class != y_true\n",
    "        cdiff = prior_probs[y_true] - predicted_probs[y_true]\n",
    "\n",
    "        print(\n",
    "            dedent(\n",
    "                \"-- TRUTH --\\n\"\n",
    "                f\"y_true={y_true}\\n\"\n",
    "                \"-- W/O FI PREDS --\\n\"\n",
    "                f\"prior_probs={prior_probs}\\n\"\n",
    "                f\"prior_class={prior_class}\\n\"\n",
    "                \"-- FI PREDS --\\n\"\n",
    "                f\"attack_results={attack_result.x}\\n\"\n",
    "                f\"predicted_probs={predicted_probs}\\n\"\n",
    "                f\"predicted_class={predicted_class}\\n\"\n",
    "                f\"success={success}\\n\"\n",
    "                f\"cdiff={cdiff}\\n\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return attack_result.x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hourly-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inputtensorfi.manipulation.img.faults import PixelFault\n",
    "from inputtensorfi.attacks.utils import attack\n",
    "\n",
    "def _look_for_pixels(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "\n",
    "    pixels = attack(\n",
    "        x,\n",
    "        y_true_index,\n",
    "        model,\n",
    "        pixel_count=1,  # Number of pixels to attack\n",
    "        maxiter=20,\n",
    "        verbose=True,\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    # Convert [x_0, y_0, r_0, g_0, b_0, x_1, ...]\n",
    "    # to [pixel_fault_0, pixel_fault_1, ...]\n",
    "    return np.array(\n",
    "        [PixelFault(*pixels[0:5]) for i in range(len(pixels) // 5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "smart-workstation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence: 0.016703619\n",
      "Confidence: 0.007902445\n",
      "Confidence: 0.0059603974\n",
      "Confidence: 0.0059603974\n",
      "Confidence: 0.0039212774\n",
      "Confidence: 0.0039212774\n",
      "Confidence: 0.0039212774\n",
      "Confidence: 0.0039212774\n",
      "Confidence: 0.0034066516\n",
      "Confidence: 0.0029714482\n",
      "Confidence: 0.0029714482\n",
      "Confidence: 0.0029391209\n",
      "Confidence: 0.0027146642\n",
      "Confidence: 0.0024395615\n",
      "Confidence: 0.0024395615\n",
      "Confidence: 0.0024395615\n",
      "Confidence: 0.0024395615\n",
      "Confidence: 0.0023783792\n",
      "Confidence: 0.0023783792\n",
      "Confidence: 0.0023783792\n",
      "-- TRUTH --\n",
      "y_true=1\n",
      "-- W/O FI PREDS --\n",
      "prior_probs=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "prior_class=1\n",
      "-- FI PREDS --\n",
      "attack_results=[22.30580396 18.43502097  2.12486956  1.75320362  1.26549577]\n",
      "predicted_probs=[8.2647735e-01 2.3783792e-03 4.1870397e-02 1.7138894e-03 4.3885365e-02\n",
      " 4.1946085e-04 7.5429659e-03 6.4170323e-02 3.2670200e-03 8.2748644e-03]\n",
      "predicted_class=0\n",
      "success=True\n",
      "cdiff=0.2986949682235718\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([PixelFault(x=22, y=18, r=2, g=1, b=1)], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id = fragile_imgs[0]\n",
    "pixels = _look_for_pixels(image_id, data_test, model)\n",
    "pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-unemployment",
   "metadata": {},
   "source": [
    "## Make a faulted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "duplicate-complement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pixel_fi_layer_tf (PixelFiLa (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10)                698154    \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from inputtensorfi import InputTensorFI\n",
    "from inputtensorfi.layers import PixelFiLayerTF\n",
    "\n",
    "faulted_model = InputTensorFI.build_faulted_model(\n",
    "    model,\n",
    "    fi_layers=[\n",
    "        PixelFiLayerTF(pixels, dtype=tf.uint8),\n",
    "    ],\n",
    ")\n",
    "faulted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-billion",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "chief-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_one(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "\n",
    "    result = model.predict(np.array([x]))[0]  # Predict one\n",
    "    result_index = np.argmax(result)\n",
    "\n",
    "    print(f\"result={result}\")\n",
    "    print(f\"result_index={result_index}\")\n",
    "    print(f\"y_true={y_true}\")\n",
    "    print(f\"y_true_index={y_true_index}\")\n",
    "    print(f\"result[y_true_index]={result[y_true_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-stuff",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "nearby-vertex",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "result_index=1\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.3010733425617218\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-durham",
   "metadata": {},
   "source": [
    "## After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "prostate-belief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n",
      "result=[8.2647735e-01 2.3783792e-03 4.1870397e-02 1.7138894e-03 4.3885365e-02\n",
      " 4.1946085e-04 7.5429659e-03 6.4170323e-02 3.2670200e-03 8.2748644e-03]\n",
      "result_index=0\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.002378379227593541\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, faulted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-violence",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "confused-details",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inputtensorfi.helpers import utils\n",
    "from inputtensorfi.manipulation.img.utils import build_perturb_image\n",
    "\n",
    "perturbated_image = build_perturb_image(pixels)(data_test[0][image_id])\n",
    "utils.plot_image(perturbated_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
