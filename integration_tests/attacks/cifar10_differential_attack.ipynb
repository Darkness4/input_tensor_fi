{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historic-dress",
   "metadata": {},
   "source": [
    "# CIFAR-10 Differential Attack\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charitable-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = os.getcwd()\n",
    "MODEL_PATH = os.path.join(FILE_PATH, \"../models/my_vgg.h5\")\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-silly",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We work with categorical (binary class matrix) instead of class vectors (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "framed-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def __prepare_datasets():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "curious-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (50000, 32, 32, 3) y_train.shape = (50000, 10)\n",
      "x_test.shape = (10000, 32, 32, 3) y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = __prepare_datasets()\n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "print(f\"x_train.shape = {x_train.shape} y_train.shape = {y_train.shape}\")\n",
    "print(f\"x_test.shape = {x_test.shape} y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-generator",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "We use our own VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from integration_tests.models.my_vgg import my_vgg\n",
    "\n",
    "def __prepare_model(data_train, data_test):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(\"---Using Existing Model---\")\n",
    "        model: tf.keras.Model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    else:\n",
    "        print(\"---Training Model---\")\n",
    "        print(f\"GPU IS AVAILABLE: {tf.config.list_physical_devices('GPU')}\")\n",
    "        model: tf.keras.Model = my_vgg()\n",
    "        model.fit(\n",
    "            *data_train,\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            validation_data=data_test,\n",
    "        )\n",
    "        model.save(MODEL_PATH)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loved-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Using Existing Model---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = __prepare_model(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-turkish",
   "metadata": {},
   "source": [
    "## Look for fragile images\n",
    "\n",
    "Images that can be easily missclassified. i.e, the binary class matrix has a low standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "powered-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_fragile_images(\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "    fragility_threshold=0.1,\n",
    "):\n",
    "    \"\"\"Look for images which are sensible to FI.\n",
    "\n",
    "    \"Fragile image\" has these conditions :\n",
    "\n",
    "    -  y_pred_index == y_true_index\n",
    "    -  std(y_pred) < fragility_threshold\n",
    "    \"\"\"\n",
    "    x_test, y_test = data_test\n",
    "    result = model.predict(x_test)\n",
    "\n",
    "    for i, y_pred in enumerate(result):\n",
    "        y_true = y_test[i]\n",
    "        y_true_index = np.argmax(y_true)\n",
    "        y_pred_index = np.argmax(y_pred)\n",
    "\n",
    "        if (\n",
    "            y_pred_index == y_true_index\n",
    "            and np.std(y_pred) < fragility_threshold\n",
    "        ):\n",
    "            print(\n",
    "                f\"image {i} is fragile.\\n\"\n",
    "                f\"std: {np.std(y_pred)}.\\n\"\n",
    "                f\"y_pred[y_true_index]={y_pred[y_true_index]}\\n\"\n",
    "                f\"y_pred[0]={y_pred[0]}\\n\"\n",
    "            )\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "failing-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 629 is fragile.\n",
      "std: 0.08297237008810043.\n",
      "y_pred[y_true_index]=0.3010731339454651\n",
      "y_pred[0]=0.11562440544366837\n",
      "\n",
      "image 878 is fragile.\n",
      "std: 0.09110084176063538.\n",
      "y_pred[y_true_index]=0.2820100784301758\n",
      "y_pred[0]=0.14032061398029327\n",
      "\n",
      "image 1975 is fragile.\n",
      "std: 0.09571876376867294.\n",
      "y_pred[y_true_index]=0.26844096183776855\n",
      "y_pred[0]=0.18283145129680634\n",
      "\n",
      "image 2032 is fragile.\n",
      "std: 0.0846942886710167.\n",
      "y_pred[y_true_index]=0.2672124207019806\n",
      "y_pred[0]=0.022509993985295296\n",
      "\n",
      "image 4282 is fragile.\n",
      "std: 0.09785398840904236.\n",
      "y_pred[y_true_index]=0.28891459107398987\n",
      "y_pred[0]=0.04903290793299675\n",
      "\n",
      "image 4705 is fragile.\n",
      "std: 0.08988036215305328.\n",
      "y_pred[y_true_index]=0.25131407380104065\n",
      "y_pred[0]=0.1394636034965515\n",
      "\n",
      "image 5700 is fragile.\n",
      "std: 0.08809787034988403.\n",
      "y_pred[y_true_index]=0.30448251962661743\n",
      "y_pred[0]=0.027622409164905548\n",
      "\n",
      "image 6083 is fragile.\n",
      "std: 0.07141575962305069.\n",
      "y_pred[y_true_index]=0.2709909975528717\n",
      "y_pred[0]=0.13926014304161072\n",
      "\n",
      "image 6729 is fragile.\n",
      "std: 0.08906044811010361.\n",
      "y_pred[y_true_index]=0.28118574619293213\n",
      "y_pred[0]=0.28118574619293213\n",
      "\n",
      "image 7491 is fragile.\n",
      "std: 0.08011265099048615.\n",
      "y_pred[y_true_index]=0.23264174163341522\n",
      "y_pred[0]=0.007820459082722664\n",
      "\n",
      "image 7735 is fragile.\n",
      "std: 0.0980263501405716.\n",
      "y_pred[y_true_index]=0.2522329092025757\n",
      "y_pred[0]=0.23485760390758514\n",
      "\n",
      "image 8428 is fragile.\n",
      "std: 0.09440356492996216.\n",
      "y_pred[y_true_index]=0.3038635849952698\n",
      "y_pred[0]=0.05074291676282883\n",
      "\n",
      "image 8480 is fragile.\n",
      "std: 0.08550631999969482.\n",
      "y_pred[y_true_index]=0.3043665587902069\n",
      "y_pred[0]=0.07086866348981857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fragile_imgs = list(__find_fragile_images(data_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-delta",
   "metadata": {},
   "source": [
    "## Differential Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-smart",
   "metadata": {},
   "source": [
    "Our implementations is based on [github.com/Hyperparticle/one-pixel-attack-keras](https://github.com/Hyperparticle/one-pixel-attack-keras).\n",
    "\n",
    "```python\n",
    "def original_perturb_image(xs, img):\n",
    "    # If this function is passed just one perturbation vector,\n",
    "    # pack it in a list to keep the computation the same\n",
    "    if xs.ndim < 2:\n",
    "        xs = np.array([xs])\n",
    "\n",
    "    # Copy the image n == len(xs) times so that we can\n",
    "    # create n new perturbed images\n",
    "    tile = [len(xs)] + [1] * (xs.ndim + 1)\n",
    "    imgs = np.tile(img, tile)\n",
    "\n",
    "    # Make sure to floor the members of xs as int types\n",
    "    xs = xs.astype(int)\n",
    "\n",
    "    for x, img in zip(xs, imgs):\n",
    "        # Split x into an array of 5-tuples (perturbation pixels)\n",
    "        # i.e., [[x,y,r,g,b], ...]\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            # At each pixel's x,y position, assign its rgb value\n",
    "            x_pos, y_pos, *rgb = pixel\n",
    "            img[x_pos, y_pos] = rgb\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def predict_classes(\n",
    "    xs: np.ndarray, img: np.ndarray, y_true: int, model: tf.keras.Model\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perturb the image and get the predictions of the model.\"\"\"\n",
    "    imgs_perturbed = original_perturb_image(xs, img)\n",
    "    predictions = model.predict(imgs_perturbed)[:, y_true]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def attack_success(\n",
    "    x: np.ndarray, img: np.ndarray, y_true: int, model: tf.keras.Model\n",
    ") -> Optional[bool]:\n",
    "    \"\"\"Predict ONE image and return True if expected. None otherwise.\"\"\"\n",
    "    attack_image = original_perturb_image(x, img)\n",
    "\n",
    "    confidence = model.predict(attack_image)[0]\n",
    "    predicted_class = np.argmax(confidence)\n",
    "\n",
    "    # If the prediction is what we want (misclassification or\n",
    "    # targeted classification), return True\n",
    "    logging.debug(\"Confidence:\", confidence[y_true])\n",
    "    if predicted_class == y_true:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def attack(\n",
    "    img: np.ndarray,\n",
    "    y_true: int,\n",
    "    model: tf.keras.Model,\n",
    "    pixel_count=1,\n",
    "    maxiter=75,\n",
    "    popsize=400,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Define bounds for a flat vector of x,y,r,g,b values\n",
    "    # For more pixels, repeat this layout\n",
    "    bounds = [(0, 32), (0, 32), (0, 256), (0, 256), (0, 256)] * pixel_count\n",
    "\n",
    "    # Population multiplier, in terms of the size of the perturbation vector x\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "\n",
    "    # Format the predict/callback functions for the differential evolution algorithm\n",
    "    def predict_fn(xs):\n",
    "        return predict_classes(xs, img, y_true, model)\n",
    "\n",
    "    def callback_fn(x, convergence):\n",
    "        return attack_success(\n",
    "            x,\n",
    "            img,\n",
    "            y_true,\n",
    "            model,\n",
    "        )\n",
    "\n",
    "    # Call Scipy's Implementation of Differential Evolution\n",
    "    attack_result = differential_evolution(\n",
    "        predict_fn,\n",
    "        bounds,\n",
    "        maxiter=maxiter,\n",
    "        popsize=popmul,\n",
    "        recombination=1,\n",
    "        atol=-1,\n",
    "        callback=callback_fn,\n",
    "        polish=False,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # Calculate some useful statistics to return from this function\n",
    "        attack_image = original_perturb_image(attack_result.x, img)[0]\n",
    "        prior_probs = model.predict(np.array([img]))[0]\n",
    "        prior_class = np.argmax(prior_probs)\n",
    "        predicted_probs = model.predict(np.array([attack_image]))[0]\n",
    "        predicted_class = np.argmax(predicted_probs)\n",
    "        success = predicted_class != y_true\n",
    "        cdiff = prior_probs[y_true] - predicted_probs[y_true]\n",
    "\n",
    "        print(\n",
    "            dedent(\n",
    "                \"-- TRUTH --\\n\"\n",
    "                f\"y_true={y_true}\\n\"\n",
    "                \"-- W/O FI PREDS --\\n\"\n",
    "                f\"prior_probs={prior_probs}\\n\"\n",
    "                f\"prior_class={prior_class}\\n\"\n",
    "                \"-- FI PREDS --\\n\"\n",
    "                f\"attack_results={attack_result.x}\\n\"\n",
    "                f\"predicted_probs={predicted_probs}\\n\"\n",
    "                f\"predicted_class={predicted_class}\\n\"\n",
    "                f\"success={success}\\n\"\n",
    "                f\"cdiff={cdiff}\\n\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return attack_result.x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sufficient-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inputtensorfi.manipulation.img.faults import PixelFault\n",
    "from inputtensorfi.attacks.utils import build_attack\n",
    "\n",
    "def _look_for_pixels(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "    \n",
    "    pixels = build_attack(\n",
    "        model, \n",
    "        pixel_count=1,  # Number of pixels to attack\n",
    "        maxiter=20,\n",
    "        verbose=True,\n",
    "    )(x, y_true_index).astype(np.uint8)\n",
    "\n",
    "    # Convert [x_0, y_0, r_0, g_0, b_0, x_1, ...]\n",
    "    # to [pixel_fault_0, pixel_fault_1, ...]\n",
    "    return np.array(\n",
    "        [PixelFault(*pixels[0:5]) for i in range(len(pixels) // 5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hired-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- TRUTH --\n",
      "y_true=1\n",
      "-- W/O FI PREDS --\n",
      "prior_probs=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "prior_class=1\n",
      "-- FI PREDS --\n",
      "attack_results=[22.68235278 18.60564503  1.83447444  0.65276946  1.3739988 ]\n",
      "predicted_probs=[8.2944256e-01 2.3131757e-03 4.1069791e-02 1.6834417e-03 4.3223035e-02\n",
      " 4.1175156e-04 7.4215163e-03 6.3091308e-02 3.2223468e-03 8.1211263e-03]\n",
      "predicted_class=0\n",
      "success=True\n",
      "cdiff=0.29876017570495605\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([PixelFault(x=22, y=18, r=1, g=0, b=1)], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id = fragile_imgs[0]\n",
    "pixels = _look_for_pixels(image_id, data_test, model)\n",
    "pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-humor",
   "metadata": {},
   "source": [
    "## Make a faulted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beautiful-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pixel_fi_layer_tf (PixelFiLa (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10)                698154    \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from inputtensorfi import InputTensorFI\n",
    "from inputtensorfi.layers import PixelFiLayerTF\n",
    "\n",
    "faulted_model = InputTensorFI.build_faulted_model(\n",
    "    model,\n",
    "    fi_layers=[\n",
    "        PixelFiLayerTF(pixels, dtype=tf.uint8),\n",
    "    ],\n",
    ")\n",
    "faulted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-mother",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minus-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_one(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "\n",
    "    result = model.predict(np.array([x]))[0]  # Predict one\n",
    "    result_index = np.argmax(result)\n",
    "\n",
    "    print(f\"result={result}\")\n",
    "    print(f\"result_index={result_index}\")\n",
    "    print(f\"y_true={y_true}\")\n",
    "    print(f\"y_true_index={y_true_index}\")\n",
    "    print(f\"result[y_true_index]={result[y_true_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-blocking",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distributed-niger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "result_index=1\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.3010733425617218\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulated-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 0.8136 - categorical_accuracy: 0.8362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8136339783668518, 0.8361999988555908)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-starter",
   "metadata": {},
   "source": [
    "## After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "radical-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[8.2944256e-01 2.3131757e-03 4.1069791e-02 1.6834417e-03 4.3223035e-02\n",
      " 4.1175156e-04 7.4215163e-03 6.3091308e-02 3.2223468e-03 8.1211263e-03]\n",
      "result_index=0\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.002313175704330206\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, faulted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "criminal-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 49s 156ms/step - loss: 0.8609 - categorical_accuracy: 0.8197\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8695204854011536, 0.8191999793052673)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = faulted_model.evaluate(x_test, y_test)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-dakota",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appreciated-medium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inputtensorfi.helpers import utils\n",
    "from inputtensorfi.manipulation.img.utils import build_perturb_image\n",
    "\n",
    "perturbated_image = build_perturb_image(pixels)(data_test[0][image_id])\n",
    "utils.plot_image(perturbated_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
