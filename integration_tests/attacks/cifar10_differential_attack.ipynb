{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "historic-dress",
   "metadata": {},
   "source": [
    "# CIFAR-10 Differential Attack\n",
    "\n",
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "opposite-calvin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "charitable-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = os.getcwd()\n",
    "MODEL_PATH = os.path.join(FILE_PATH, \"../models/my_vgg.h5\")\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-silly",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We work with categorical (binary class matrix) instead of class vectors (integers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "framed-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def __prepare_datasets():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "curious-saturday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (50000, 32, 32, 3) y_train.shape = (50000, 10)\n",
      "x_test.shape = (10000, 32, 32, 3) y_test.shape = (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_test = __prepare_datasets()\n",
    "x_train, y_train = data_train\n",
    "x_test, y_test = data_test\n",
    "print(f\"x_train.shape = {x_train.shape} y_train.shape = {y_train.shape}\")\n",
    "print(f\"x_test.shape = {x_test.shape} y_test.shape = {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-generator",
   "metadata": {},
   "source": [
    "## Model preparation\n",
    "\n",
    "We use our own VGG model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "coupled-accuracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from integration_tests.models.my_vgg import my_vgg\n",
    "\n",
    "def __prepare_model(data_train, data_test):\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(\"---Using Existing Model---\")\n",
    "        model: tf.keras.Model = tf.keras.models.load_model(MODEL_PATH)\n",
    "    else:\n",
    "        print(\"---Training Model---\")\n",
    "        print(f\"GPU IS AVAILABLE: {tf.config.list_physical_devices('GPU')}\")\n",
    "        model: tf.keras.Model = my_vgg()\n",
    "        model.fit(\n",
    "            *data_train,\n",
    "            epochs=100,\n",
    "            batch_size=64,\n",
    "            validation_data=data_test,\n",
    "        )\n",
    "        model.save(MODEL_PATH)\n",
    "\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "loved-empire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Using Existing Model---\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = __prepare_model(data_train, data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-turkish",
   "metadata": {},
   "source": [
    "## Look for fragile images\n",
    "\n",
    "Images that can be easily missclassified. i.e, the binary class matrix has a low standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "powered-spare",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __find_fragile_images(\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "    fragility_threshold=0.1,\n",
    "):\n",
    "    \"\"\"Look for images which are sensible to FI.\n",
    "\n",
    "    \"Fragile image\" has these conditions :\n",
    "\n",
    "    -  y_pred_index == y_true_index\n",
    "    -  std(y_pred) < fragility_threshold\n",
    "    \"\"\"\n",
    "    x_test, y_test = data_test\n",
    "    result = model.predict(x_test)\n",
    "\n",
    "    for i, y_pred in enumerate(result):\n",
    "        y_true = y_test[i]\n",
    "        y_true_index = np.argmax(y_true)\n",
    "        y_pred_index = np.argmax(y_pred)\n",
    "\n",
    "        if (\n",
    "            y_pred_index == y_true_index\n",
    "            and np.std(y_pred) < fragility_threshold\n",
    "        ):\n",
    "            print(\n",
    "                f\"image {i} is fragile.\\n\"\n",
    "                f\"std: {np.std(y_pred)}.\\n\"\n",
    "                f\"y_pred[y_true_index]={y_pred[y_true_index]}\\n\"\n",
    "                f\"y_pred[0]={y_pred[0]}\\n\"\n",
    "            )\n",
    "            yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "failing-water",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 629 is fragile.\n",
      "std: 0.08297237008810043.\n",
      "y_pred[y_true_index]=0.3010731339454651\n",
      "y_pred[0]=0.11562440544366837\n",
      "\n",
      "image 878 is fragile.\n",
      "std: 0.09110084176063538.\n",
      "y_pred[y_true_index]=0.2820100784301758\n",
      "y_pred[0]=0.14032061398029327\n",
      "\n",
      "image 1975 is fragile.\n",
      "std: 0.09571876376867294.\n",
      "y_pred[y_true_index]=0.26844096183776855\n",
      "y_pred[0]=0.18283145129680634\n",
      "\n",
      "image 2032 is fragile.\n",
      "std: 0.0846942886710167.\n",
      "y_pred[y_true_index]=0.2672124207019806\n",
      "y_pred[0]=0.022509993985295296\n",
      "\n",
      "image 4282 is fragile.\n",
      "std: 0.09785398840904236.\n",
      "y_pred[y_true_index]=0.28891459107398987\n",
      "y_pred[0]=0.04903290793299675\n",
      "\n",
      "image 4705 is fragile.\n",
      "std: 0.08988036215305328.\n",
      "y_pred[y_true_index]=0.25131407380104065\n",
      "y_pred[0]=0.1394636034965515\n",
      "\n",
      "image 5700 is fragile.\n",
      "std: 0.08809787034988403.\n",
      "y_pred[y_true_index]=0.30448251962661743\n",
      "y_pred[0]=0.027622409164905548\n",
      "\n",
      "image 6083 is fragile.\n",
      "std: 0.07141575962305069.\n",
      "y_pred[y_true_index]=0.2709909975528717\n",
      "y_pred[0]=0.13926014304161072\n",
      "\n",
      "image 6729 is fragile.\n",
      "std: 0.08906044811010361.\n",
      "y_pred[y_true_index]=0.28118574619293213\n",
      "y_pred[0]=0.28118574619293213\n",
      "\n",
      "image 7491 is fragile.\n",
      "std: 0.08011265099048615.\n",
      "y_pred[y_true_index]=0.23264174163341522\n",
      "y_pred[0]=0.007820459082722664\n",
      "\n",
      "image 7735 is fragile.\n",
      "std: 0.0980263501405716.\n",
      "y_pred[y_true_index]=0.2522329092025757\n",
      "y_pred[0]=0.23485760390758514\n",
      "\n",
      "image 8428 is fragile.\n",
      "std: 0.09440356492996216.\n",
      "y_pred[y_true_index]=0.3038635849952698\n",
      "y_pred[0]=0.05074291676282883\n",
      "\n",
      "image 8480 is fragile.\n",
      "std: 0.08550631999969482.\n",
      "y_pred[y_true_index]=0.3043665587902069\n",
      "y_pred[0]=0.07086866348981857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fragile_imgs = list(__find_fragile_images(data_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-delta",
   "metadata": {},
   "source": [
    "## Differential Attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-smart",
   "metadata": {},
   "source": [
    "Our implementations is based on [github.com/Hyperparticle/one-pixel-attack-keras](https://github.com/Hyperparticle/one-pixel-attack-keras).\n",
    "\n",
    "```python\n",
    "def original_perturb_image(xs, img):\n",
    "    # If this function is passed just one perturbation vector,\n",
    "    # pack it in a list to keep the computation the same\n",
    "    if xs.ndim < 2:\n",
    "        xs = np.array([xs])\n",
    "\n",
    "    # Copy the image n == len(xs) times so that we can\n",
    "    # create n new perturbed images\n",
    "    tile = [len(xs)] + [1] * (xs.ndim + 1)\n",
    "    imgs = np.tile(img, tile)\n",
    "\n",
    "    # Make sure to floor the members of xs as int types\n",
    "    xs = xs.astype(int)\n",
    "\n",
    "    for x, img in zip(xs, imgs):\n",
    "        # Split x into an array of 5-tuples (perturbation pixels)\n",
    "        # i.e., [[x,y,r,g,b], ...]\n",
    "        pixels = np.split(x, len(x) // 5)\n",
    "        for pixel in pixels:\n",
    "            # At each pixel's x,y position, assign its rgb value\n",
    "            x_pos, y_pos, *rgb = pixel\n",
    "            img[x_pos, y_pos] = rgb\n",
    "\n",
    "    return imgs\n",
    "\n",
    "\n",
    "def predict_classes(\n",
    "    xs: np.ndarray, img: np.ndarray, y_true: int, model: tf.keras.Model\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Perturb the image and get the predictions of the model.\"\"\"\n",
    "    imgs_perturbed = original_perturb_image(xs, img)\n",
    "    predictions = model.predict(imgs_perturbed)[:, y_true]\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def attack_success(\n",
    "    x: np.ndarray, img: np.ndarray, y_true: int, model: tf.keras.Model\n",
    ") -> Optional[bool]:\n",
    "    \"\"\"Predict ONE image and return True if expected. None otherwise.\"\"\"\n",
    "    attack_image = original_perturb_image(x, img)\n",
    "\n",
    "    confidence = model.predict(attack_image)[0]\n",
    "    predicted_class = np.argmax(confidence)\n",
    "\n",
    "    # If the prediction is what we want (misclassification or\n",
    "    # targeted classification), return True\n",
    "    logging.debug(\"Confidence:\", confidence[y_true])\n",
    "    if predicted_class == y_true:\n",
    "        return True\n",
    "\n",
    "    \n",
    "def attack(\n",
    "    img: np.ndarray,\n",
    "    y_true: int,\n",
    "    model: tf.keras.Model,\n",
    "    pixel_count=1,\n",
    "    maxiter=75,\n",
    "    popsize=400,\n",
    "    verbose=False,\n",
    "):\n",
    "    # Define bounds for a flat vector of x,y,r,g,b values\n",
    "    # For more pixels, repeat this layout\n",
    "    bounds = [(0, 32), (0, 32), (0, 256), (0, 256), (0, 256)] * pixel_count\n",
    "\n",
    "    # Population multiplier, in terms of the size of the perturbation vector x\n",
    "    popmul = max(1, popsize // len(bounds))\n",
    "\n",
    "    # Format the predict/callback functions for the differential evolution algorithm\n",
    "    def predict_fn(xs):\n",
    "        return predict_classes(xs, img, y_true, model)\n",
    "\n",
    "    def callback_fn(x, convergence):\n",
    "        return attack_success(\n",
    "            x,\n",
    "            img,\n",
    "            y_true,\n",
    "            model,\n",
    "        )\n",
    "\n",
    "    # Call Scipy's Implementation of Differential Evolution\n",
    "    attack_result = differential_evolution(\n",
    "        predict_fn,\n",
    "        bounds,\n",
    "        maxiter=maxiter,\n",
    "        popsize=popmul,\n",
    "        recombination=1,\n",
    "        atol=-1,\n",
    "        callback=callback_fn,\n",
    "        polish=False,\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        # Calculate some useful statistics to return from this function\n",
    "        attack_image = original_perturb_image(attack_result.x, img)[0]\n",
    "        prior_probs = model.predict(np.array([img]))[0]\n",
    "        prior_class = np.argmax(prior_probs)\n",
    "        predicted_probs = model.predict(np.array([attack_image]))[0]\n",
    "        predicted_class = np.argmax(predicted_probs)\n",
    "        success = predicted_class != y_true\n",
    "        cdiff = prior_probs[y_true] - predicted_probs[y_true]\n",
    "\n",
    "        print(\n",
    "            dedent(\n",
    "                \"-- TRUTH --\\n\"\n",
    "                f\"y_true={y_true}\\n\"\n",
    "                \"-- W/O FI PREDS --\\n\"\n",
    "                f\"prior_probs={prior_probs}\\n\"\n",
    "                f\"prior_class={prior_class}\\n\"\n",
    "                \"-- FI PREDS --\\n\"\n",
    "                f\"attack_results={attack_result.x}\\n\"\n",
    "                f\"predicted_probs={predicted_probs}\\n\"\n",
    "                f\"predicted_class={predicted_class}\\n\"\n",
    "                f\"success={success}\\n\"\n",
    "                f\"cdiff={cdiff}\\n\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return attack_result.x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sufficient-needle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inputtensorfi.manipulation.img.faults import PixelFault\n",
    "from inputtensorfi.attacks.utils import attack\n",
    "\n",
    "def _look_for_pixels(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "\n",
    "    pixels = attack(\n",
    "        x,\n",
    "        y_true_index,\n",
    "        model,\n",
    "        pixel_count=1,  # Number of pixels to attack\n",
    "        maxiter=20,\n",
    "        verbose=True,\n",
    "    ).astype(np.uint8)\n",
    "\n",
    "    # Convert [x_0, y_0, r_0, g_0, b_0, x_1, ...]\n",
    "    # to [pixel_fault_0, pixel_fault_1, ...]\n",
    "    return np.array(\n",
    "        [PixelFault(*pixels[0:5]) for i in range(len(pixels) // 5)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "hired-matter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- TRUTH --\n",
      "y_true=1\n",
      "-- W/O FI PREDS --\n",
      "prior_probs=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "prior_class=1\n",
      "-- FI PREDS --\n",
      "attack_results=[ 22.70049911  18.26658799 254.26148625 254.86461561 255.51944075]\n",
      "predicted_probs=[8.9320129e-01 2.5950614e-03 2.1584732e-02 1.1265812e-03 2.5294218e-02\n",
      " 2.7755674e-04 6.3416907e-03 3.7432037e-02 3.0920343e-03 9.0547530e-03]\n",
      "predicted_class=0\n",
      "success=True\n",
      "cdiff=0.29847827553749084\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([PixelFault(x=22, y=18, r=254, g=254, b=255)], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_id = fragile_imgs[0]\n",
    "pixels = _look_for_pixels(image_id, data_test, model)\n",
    "pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-humor",
   "metadata": {},
   "source": [
    "## Make a faulted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "beautiful-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "pixel_fi_layer_tf (PixelFiLa (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 10)                698154    \n",
      "=================================================================\n",
      "Total params: 698,154\n",
      "Trainable params: 698,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from inputtensorfi import InputTensorFI\n",
    "from inputtensorfi.layers import PixelFiLayerTF\n",
    "\n",
    "faulted_model = InputTensorFI.build_faulted_model(\n",
    "    model,\n",
    "    fi_layers=[\n",
    "        PixelFiLayerTF(pixels, dtype=tf.uint8),\n",
    "    ],\n",
    ")\n",
    "faulted_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "african-mother",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "minus-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate_one(\n",
    "    image_id: int,\n",
    "    data_test: np.ndarray,\n",
    "    model: tf.keras.Model,\n",
    "):\n",
    "    x_test, y_test = data_test\n",
    "    x = x_test[image_id]\n",
    "    y_true = y_test[image_id]\n",
    "    y_true_index = np.argmax(y_true)\n",
    "\n",
    "    result = model.predict(np.array([x]))[0]  # Predict one\n",
    "    result_index = np.argmax(result)\n",
    "\n",
    "    print(f\"result={result}\")\n",
    "    print(f\"result_index={result_index}\")\n",
    "    print(f\"y_true={y_true}\")\n",
    "    print(f\"y_true_index={y_true_index}\")\n",
    "    print(f\"result[y_true_index]={result[y_true_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-blocking",
   "metadata": {},
   "source": [
    "## Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "distributed-niger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[0.1156243  0.30107334 0.10992888 0.01272675 0.08838192 0.00309303\n",
      " 0.02038096 0.15960985 0.07248399 0.116697  ]\n",
      "result_index=1\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.3010733425617218\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "regulated-documentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.8136 - categorical_accuracy: 0.8362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8136339783668518, 0.8361999988555908)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-starter",
   "metadata": {},
   "source": [
    "## After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "radical-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result=[8.9320129e-01 2.5950614e-03 2.1584732e-02 1.1265812e-03 2.5294218e-02\n",
      " 2.7755674e-04 6.3416907e-03 3.7432037e-02 3.0920343e-03 9.0547530e-03]\n",
      "result_index=0\n",
      "y_true=[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_true_index=1\n",
      "result[y_true_index]=0.0025950614362955093\n"
     ]
    }
   ],
   "source": [
    "_evaluate_one(image_id, data_test, faulted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "criminal-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting TensorScatterUpdate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 10s 29ms/step - loss: 0.9234 - categorical_accuracy: 0.8006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9265120029449463, 0.8015000224113464)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc = faulted_model.evaluate(x_test, y_test)\n",
    "loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-dakota",
   "metadata": {},
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appreciated-medium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from inputtensorfi.helpers import utils\n",
    "from inputtensorfi.manipulation.img.utils import build_perturb_image\n",
    "\n",
    "perturbated_image = build_perturb_image(pixels)(data_test[0][image_id])\n",
    "utils.plot_image(perturbated_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
